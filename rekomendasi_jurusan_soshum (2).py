# -*- coding: utf-8 -*-
"""Rekomendasi Jurusan Soshum.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/14UY5D_ocZ4svmKJ9IWe3bnmG26gfTFWM

#Proyek Akhir : Membuat Model Sistem Rekomendasi

# Deskripsi Proyek

# Latar Belakang

Pemilihan jurusan perguruan tinggi merupakan salah satu keputusan penting bagi siswa yang baru lulus sekolah menengah atas. Terutama pada bidang sosial dan humaniora (soshum), banyak siswa yang masih bingung menentukan jurusan yang sesuai dengan minat dan kemampuan mereka. Kesalahan dalam memilih jurusan dapat berdampak negatif pada motivasi belajar dan prestasi akademik.

Seiring dengan perkembangan teknologi dan tersedianya data yang beragam, sistem rekomendasi menjadi solusi efektif untuk membantu siswa dalam pengambilan keputusan pemilihan jurusan. Pada proyek ini, digunakan tiga dataset utama, yaitu data universitas (universities), data jurusan (majors), dan skor UTBK bidang sosial dan humaniora (score humanities). Ketiga dataset tersebut menjadi dasar dalam mengembangkan sistem rekomendasi yang lebih personal dan relevan bagi setiap siswa.

Sistem rekomendasi ini menggabungkan dua pendekatan utama, yaitu Content-Based Filtering yang merekomendasikan jurusan berdasarkan kesamaan fitur jurusan yang diminati pengguna, dan Collaborative Filtering yang memanfaatkan pola preferensi pengguna lain dengan minat serupa. Dengan penggabungan kedua teknik tersebut, diharapkan sistem dapat memberikan rekomendasi jurusan soshum yang sesuai dengan potensi dan preferensi calon mahasiswa, sehingga meningkatkan peluang keberhasilan akademik di masa depan.

# Import Library
Pada tahap awal, berbagai pustaka diimpor untuk mendukung analisis data dan pembangunan model. Pandas dan NumPy digunakan untuk manipulasi data, sementara seaborn, matplotlib, dan plotly digunakan untuk visualisasi. TensorFlow dan Keras membantu membangun model pembelajaran mesin, dan scikit-learn digunakan untuk pemrosesan teks seperti TF-IDF dan cosine similarity. Beberapa pustaka tambahan seperti tabulate dan pathlib juga digunakan untuk mempermudah tampilan dan pengelolaan file.
"""

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
import plotly.express as px
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
from tabulate import tabulate
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
from pathlib import Path

"""# Data Understanding
Data Understanding adalah tahap untuk memahami dataset secara mendalam.

# Data Loading
Data diambil dari Kaggle dengan judul Indonesia College Entrance Examination - UTBK 2019, yang dikumpulkan oleh Eko J. Salim. Dari empat dataset yang tersedia, proyek ini menggunakan tiga yang paling relevan, yaitu:

  - Major: informasi tentang program studi/jurusan.

  - Score_humanities: data skor UTBK peserta di bidang soshum.

  - Universities: daftar universitas tujuan peserta.

Ketiga dataset ini menjadi dasar dalam membangun sistem rekomendasi berdasarkan minat dan skor peserta.
"""

# Import module yang disediakan google colab untuk kebutuhan upload file
from google.colab import files
files.upload()

!mkdir -p ~/.kaggle
!cp kaggle.json ~/.kaggle/
!chmod 600 ~/.kaggle/kaggle.json

!kaggle datasets download -d ekojsalim/indonesia-college-entrance-examination-utbk-2019
!unzip indonesia-college-entrance-examination-utbk-2019.zip

"""# Read data

* Universitas
"""

universitas = pd.read_csv('/content/universities.csv')
universitas

"""dari informasi diatas dapat disimpulkan bahwa :

  * Dataset berisi 85 baris (rows), yang masing-masing mewakili satu sampel atau data universitas.

  * Terdapat 3 kolom (columns) yaitu fitur dan informasi sebagai berikut:
    - Unnamed : Kolom tanpa nama yang kemungkinan merupakan indeks otomatis dari sistem atau dapat diabaikan jika tidak mengandung informasi penting.

    - id_university : ID unik untuk setiap universitas.

    - university_name : Nama universitas.
"""

# Cek missing value
universitas.isnull().sum()

"""* Major"""

major = pd.read_csv('/content/majors.csv')
major

"""dari informasi diatas dapat disimpulkan bahwa :
  - Unnamed: 0 : Kolom tanpa nama yang kemungkinan merupakan indeks otomatis atau hasil ekspor dari file spreadsheet. Kolom ini bisa diabaikan jika tidak mengandung informasi penting.

  - id_major : ID unik untuk setiap jurusan.

  - id_university : ID dari universitas tempat jurusan tersebut berada, yang bisa digunakan untuk menghubungkan ke dataset universitas.

  - type : Jenis jurusan, misalnya bisa berupa program studi reguler, internasional, vokasi, dll. (tergantung isi data).

  - major_name : Nama jurusan, seperti Teknik Informatika, Kedokteran, Hukum, dan lain-lain.

  - capacity : Kapasitas daya tampung jurusan, menunjukkan jumlah maksimum mahasiswa yang dapat diterima.
"""

# Cek Missing Value
major.isnull().sum()

"""* score_humanities"""

score_humanities = pd.read_csv('/content/score_humanities.csv')
score_humanities

"""Dari output diatas dapat disimpulkan bahwa :
* Dataset berisi 61.202 baris (rows), yang masing-masing mewakili satu data pendaftaran atau pilihan jurusan dari seorang pengguna (calon mahasiswa).

* Terdapat 15 kolom (columns) yaitu fitur-fitur sebagai berikut:

  - Unnamed: 0 : Kolom tanpa nama yang kemungkinan merupakan indeks otomatis dari sistem atau hasil ekspor file, dan biasanya tidak mengandung informasi penting.
  - id_first_major : ID jurusan pilihan pertama yang dipilih oleh pengguna.
  - id_first_university : ID universitas dari jurusan pilihan pertama.
  - id_second_major : ID jurusan pilihan kedua.
  - id_second_university : ID universitas dari jurusan pilihan kedua.
  - id_user : ID unik yang merepresentasikan pengguna atau calon mahasiswa.
  - score_eko : Skor pengguna pada mata pelajaran Ekonomi.
  - score_geo : Skor pengguna pada mata pelajaran Geografi.
  - score_kmb : Skor Kemampuan Berpikir Matematis dan Berlogika.
  - score_kpu : Skor Kemampuan Penalaran Umum.
  - score_kua : Skor Kemampuan Verbal (Bahasa Indonesia dan Bahasa Inggris).
  - score_mat : Skor Matematika.
  - score_ppu : Skor Pengetahuan dan Pemahaman Umum.
  - score_sej : Skor pengguna pada mata pelajaran Sejarah.
  - score_sos : Skor rata-rata untuk kelompok mata pelajaran sosial.
"""

# Cek Missing Value
score_humanities.isnull().sum()

"""# Cek Missing Value

# Exploratory Data Analysis (EDA)
Exploratory data analysis adalah tahap awal untuk memahami data, menemukan pola, serta mendeteksi anomali menggunakan statistik dan visualisasi.

# Univariate Exploratory Data Analysis

# EDA | Universitas
Dataset ini hanya terdiri dari dua variabel, yaitu id_university dan university_name. Berikut merupakan visualisasi dari isi dataset tersebut.

# 1. Cek Karakteristik Dataset
"""

universitas.describe()

"""dari output diatas dapat disimpulkan bahwa :

- Count menunjukkan jumlah data, yaitu sebanyak 85 universitas.
- Mean merupakan nilai rata-rata ID universitas, yaitu sebesar 411,41.
- Standard deviation (std) menunjukkan tingkat sebaran data dari nilai rata-rata, yaitu sebesar 230,86.
- Min adalah nilai ID terkecil, yaitu 111.
- Kuartil pertama (25%) menunjukkan bahwa 25 persen data memiliki ID kurang dari atau sama dengan 192.
- Median (50%) menunjukkan bahwa 50 persen data memiliki ID kurang dari atau sama dengan 361.
- Kuartil ketiga (75%) menunjukkan bahwa 75 persen data memiliki ID kurang dari atau sama dengan 611.
- Max adalah nilai ID terbesar, yaitu 921.

# 2. Cek nilai Unik
"""

# cek nilai unik
print('Jumlah uniq id universitas : ', len(universitas["id_university"].unique()))
print('Jumlah uniq nama universitas : ', len(universitas["university_name"].unique()))

"""Setelah dilakukan pengecekan, dapat disimpulkan bahwa jumlah nilai unik pada masing-masing kolom adalah 85.

# EDA | Major

# 1. Cek Karakteristik Dataset
"""

major.describe()

"""Dari output di atas dapat disimpulkan bahwa:

- Count menunjukkan jumlah data valid pada setiap kolom, yaitu sebanyak 3167 data.
- Mean merupakan nilai rata-rata dari setiap kolom numerik.
- Std menunjukkan standar deviasi, yaitu tingkat sebaran data terhadap nilai rata-rata.
- Min dan max menunjukkan nilai terkecil dan terbesar pada setiap kolom.
- Nilai 25%, 50% (median), dan 75% merupakan kuartil data yang menggambarkan distribusi data dalam persentil.

# 2. Cek Nilai Unik
"""

# cek nilai unik
print('Jumlah uniq id universitas : ', len(major["id_university"].unique()))
print('Jumlah uniq id major : ', len(major["id_major"].unique()))
print('Jumlah uniq kapasitas : ', len(major["capacity"].unique()))

"""kesimpulan:
- Kolom id_university memiliki 85 nilai unik, yang berarti ada 85 universitas berbeda dalam data.
- Kolom id_major memiliki 3167 nilai unik, yang menunjukkan setiap jurusan memiliki ID yang unik tanpa duplikasi.
- Kolom capacity memiliki 143 nilai unik, yang menunjukkan variasi kapasitas daya tampung pada jurusan-jurusan tersebut.

# EDA | Score Humanities

# 1. Cek Nilai Unik
"""

score_humanities.info()

# cek nilai unik
score_humanities[['id_first_major',
                  'id_first_university',
                  'id_second_major',
                  'id_second_university',
                  'id_user']].nunique()

"""kesimpulan:
- id_first_major: Terdapat 1.290 program studi unik yang dipilih sebagai pilihan pertama.

- id_first_university: Sebanyak 87 universitas unik tercatat sebagai pilihan pertama.

- id_second_major: Terdapat 1.353 program studi unik sebagai pilihan kedua, menunjukkan variasi yang lebih besar dibanding pilihan pertama.

- id_second_university: Pilihan kedua berasal dari 86 universitas berbeda.

- id_user: Terdapat 61.202 nilai unik, artinya setiap baris mewakili satu pengguna yang berbeda (tidak ada duplikasi).

# 2. Cek Karakteristik Dataset
"""

score_humanities.describe()

"""kesimpulan:
- Count menunjukkan jumlah data valid, yaitu 61.202 baris untuk setiap kolom.

- Mean adalah nilai rata-rata untuk setiap kolom, misalnya rata-rata skor mata pelajaran berkisar antara 520 sampai 552.

- Std (standar deviasi) menggambarkan sebaran data dari nilai rata-rata.

- Min dan Max menunjukkan nilai terkecil dan terbesar di setiap kolom, misalnya skor mata pelajaran minimal 0 dan maksimal ada yang mencapai lebih dari 1.000.

- 25%, 50% (median), dan 75% adalah kuartil yang menunjukkan distribusi data dalam persentil, memberikan gambaran nilai tengah dan sebaran skor.

# Data Preparation Umum

# 1. Penghapusan Kolom
penghapusan kolom ini bertujuan untuk membersihkan data agar lebih rapi dan fokus pada informasi yang relevan.
"""

# drop kolom
universitas = universitas.drop(['Unnamed: 0'], axis=1)
major = major.drop(['Unnamed: 0'], axis=1)
score_humanities = score_humanities.drop(['Unnamed: 0', 'id_second_major', 'id_second_university'], axis=1)

"""# 2. Rename Nama Kolom pada Dataset Score_humanities
Pada proses ini, dilakukan penggantian nama kolom untuk memudahkan proses penggabungan data dan analisis selanjutnya.
"""

score_humanities = score_humanities.rename(columns={"id_first_major" : "id_major", "id_first_university" : "id_university"})
score_humanities.head(5)

"""# 3. Menghitung Nilai Rata-rata pada Dataset Score_Humanities
Langkah selanjutnya adalah menghitung nilai rata-rata dari beberapa kolom pada score_humanities. Tujuan dari proses ini adalah menyederhanakan data sehingga analisis dan pemrosesan selanjutnya dapat dilakukan dengan lebih mudah menggunakan satu kolom rata-rata saja.
"""

score_humanities['rata_rata_nilai'] = score_humanities[['score_eko', 'score_geo', 'score_kmb',
                             'score_kpu', 'score_kua', 'score_mat', 'score_ppu', 'score_sej', 'score_sos']].mean(axis=1)

score_humanities.drop(['score_eko', 'score_geo', 'score_kmb',
                             'score_kpu', 'score_kua', 'score_mat', 'score_ppu', 'score_sej', 'score_sos'], axis=1, inplace=True)

# Menampilkan dataframe dengan rata-rata nilai baru
score_humanities

"""# 4. Menggabungkan 3 Dataset
proses selanjutnya adalah menggabungkan 3 dataset yaitu universitas, major, dan score_humanities agar lebih mudah dianalisis.
"""

# menggabungkan 3 dataset yaitu score_humanities, major dan univ
merged_data = pd.merge(score_humanities, major, on='id_major', how='left')
merged_data = pd.merge(merged_data, universitas[['id_university', 'university_name']], left_on='id_university_x', right_on='id_university', how='left')

# Hapus kolom 'id_university_x' dan 'id_university_y'
merged_data.drop(['id_university_x', 'id_university_y'], axis=1, inplace=True)
merged_data

"""# 5. Data Filtering"""

# Menghapus baris dengan nilai 'science' pada kolom 'type'
merged_data_clean = merged_data[merged_data['type'] != 'science']

# Menampilkan DataFrame hasil penghapusan baris
merged_data_clean

"""# 6. Penanganan Missing Vlaue
Pengecekan dan penghapusan kolom yang mengandung missing value dilakukan untuk menjaga kualitas data, mencegah bias, dan memastikan model dapat dilatih dengan data yang lengkap dan konsisten.
"""

# Cek missing value dengan fungsi isnull()
merged_data_clean.isnull().sum()

sns.heatmap(merged_data_clean.isna(), cmap='coolwarm')

sns.heatmap(merged_data_clean.isna(), cmap='plasma')

# Membersihkan missing value dengan fungsi dropna()
merged_data_clean = merged_data_clean.dropna()
merged_data_clean.head()

# mengecek ulang missing value
merged_data_clean.isnull().sum()

"""# 7. Penanganan Kolom Duplikat
Pengecekan dan penghapusan kolom duplikat dilakukan untuk memastikan tidak ada data yang berulang, sehingga analisis menjadi lebih akurat dan efisien tanpa adanya informasi yang berlebihan atau redundan.
"""

# cek duplikat
preparation = merged_data_clean
preparation.sort_values('id_major')

# Membuang data duplikat pada variabel preparation
p = preparation.drop_duplicates('id_major')
p

"""# Data Preparation - Content Based Filtering

# 1. Konversi Data Series Menjadi List
proses selanjutnya adalah melakukan konversi kolom id_major, university_name, dan major_name dari format Series menjadi list untuk memudahkan proses pengolahan data selanjutnya. Selanjutnya, dilakukan pengecekan panjang list untuk memastikan data sudah lengkap
"""

# Mengonversi data series id_major menjadi dalam bentuk list
id_major = p['id_major'].tolist()
# Mengonversi data series university_name menjadi dalam bentuk list
nama_Univ = p['university_name'].tolist()
# Mengonversi data series major_name menjadi dalam bentuk list
nama_Prodi = p['major_name'].tolist()

print(len(id_major))
print(len(nama_Univ))
print(len(nama_Prodi))

"""**Insight**

Hasil output menunjukkan bahwa ketiga list memiliki panjang yang sama, yaitu 1.286 elemen, yang menunjukkan kesesuaian struktur data dan tidak adanya kehilangan data selama proses konversi. Data dalam format list ini siap digunakan untuk proses selanjutnya, seperti pembuatan rekomendasi jurusan atau visualisasi berbasis array.

# 2. Membuat Dictionary
proses selanjutnya adalah Membuat DataFrame baru yang berisi kolom id_major, university_name, dan major_name dari list yang telah dikonversi sebelumnya untuk memudahkan pengelolaan dan analisis data.
"""

# Membuat dictionary untuk data id_major, nama_Univ, dan nama_Prodi
id_new = pd.DataFrame({
    'id_major': id_major,
    'university_name': nama_Univ,
    'major_name': nama_Prodi

})
id_new

# mengecek dataframe final
data = id_new
data.sample(5)

"""# 3. TF-IDF Vectorizer
Metode evaluasi ini, yang disebut TF-IDF (Term Frequency-Inverse Document Frequency), digunakan untuk mengukur pentingnya suatu kata dibandingkan dengan kata-kata lain dalam sebuah dokumen. Secara matematis, TF-IDF terdiri dari dua komponen utama: TF (Term Frequency) yang menunjukkan frekuensi kemunculan suatu kata dalam sebuah teks, dan IDF (Inverse Document Frequency) yang menunjukkan seberapa jarang atau umumnya kata tersebut muncul di seluruh kumpulan dokumen.
"""

# Inisialisasi TfidfVectorizer
tfidf_vectorizer = TfidfVectorizer()

# Melakukan perhitungan idf pada data major_name
tfidf_vectorizer.fit(data['major_name'])

# Mapping array dari fitur index integer ke fitur nama
tfidf_vectorizer.get_feature_names_out()

# Melakukan fit lalu ditransformasikan ke bentuk matrix
tfidf_matrix = tfidf_vectorizer.fit_transform(data['major_name'])

# Melihat ukuran matrix tfidf
tfidf_matrix.shape

"""kemudian melakukan pelatihan dan transformasi data teks pada kolom major_name menggunakan TfidfVectorizer sehingga menghasilkan matriks TF-IDF. Kemudian, ukuran matriks tersebut diperiksa untuk mengetahui jumlah dokumen dan fitur yang dihasilkan"""

# Mengubah vektor tf-idf dalam bentuk matriks dengan fungsi todense()
tfidf_matrix.todense()

# Membuat dataframe untuk melihat tf-idf matrix
# Kolom diisi dengan jenis masakan
# Baris diisi dengan nama resto

pd.DataFrame(
    tfidf_matrix.todense(),
    columns=tfidf_vectorizer.get_feature_names_out(),
    index=data.id_major
).sample(22, axis=1).sample(10, axis=0)

"""# Data Preparation - Collaborative Filtering

# Data Preparation
"""

# Membaca dataset
df = p
df

"""# 1. Encode
proses selanjutnya adalah mengubah userID menjadi angka yang berurutan agar lebih mudah saat akan digunakan
"""

# Mengubah userID menjadi list tanpa nilai yang sama
id_peserta = df['id_user'].unique().tolist()
print('list userID: ', id_peserta)

# Melakukan encoding id_peserta
user_to_user_encoded = {x: i for i, x in enumerate(id_peserta)}
print('encoded userID : ', user_to_user_encoded)

# Melakukan proses encoding angka ke ke id_univ
user_encoded_to_user = {i: x for i, x in enumerate(id_peserta)}
print('encoded angka ke userID: ',user_encoded_to_user)

# Mengubah userID menjadi list tanpa nilai yang sama
code_prodi = df['id_major'].unique().tolist()
print('list userID: ', code_prodi)

# Melakukan encoding code_prodi
prodi_to_prodi_encoded = {x: i for i, x in enumerate(code_prodi)}
print('encoded userID : ', prodi_to_prodi_encoded)

# Melakukan proses encoding angka ke ke code_prodi
prodi_encoded_to_prodi = {i: x for i, x in enumerate(code_prodi)}
print('encoded angka ke userID: ', prodi_encoded_to_prodi)

"""# 2. Mapping Features
Pada tahap ini, nilai-nilai asli pada kolom id_user dan id_major dalam DataFrame df diubah menjadi bentuk numerik (encoded). Proses ini bertujuan untuk mempermudah pemrosesan data pada tahapan analisis dan pemodelan selanjutnya, terutama ketika digunakan dalam algoritma machine learning yang memerlukan input dalam format numerik.
"""

# Mapping user ke dataframe id_user
df['user'] = df['id_user'].map(user_to_user_encoded)
# Mapping prodi ke dataframe id_major
df['prodi'] = df['id_major'].map(prodi_to_prodi_encoded)

# Mendapatkan jumlah iser
num_user = len(user_to_user_encoded)
print(num_user)

# Mendapatkan jumlah prodi
num_prodi = len(prodi_encoded_to_prodi)
print(num_prodi)

# Nilai minimum hasil rata-rata nilai tes mahasiswa
min_nilai_mah = min(df['rata_rata_nilai'])

# Nilai maksimal hasil rata-rata nilai tes mahasiswa
max_nilai_mah = max(df['rata_rata_nilai'])

print('Number of user: {}, Number of prodi: {}, Min nilai hasil rata-rata tes: {}, Max nilai hasil rata-rata tes : {}'.format(
    num_user, num_prodi, min_nilai_mah, max_nilai_mah
))

"""# 3. Membagi Data untuk Training dan Validasi
Data pada dataframe df diacak terlebih dahulu, kemudian dipisahkan menjadi dua bagian: fitur (x), yang terdiri dari kolom 'user' dan 'prodi', serta target (y), yaitu kolom 'rata_rata_nilai'. Selanjutnya, nilai pada kolom 'rata_rata_nilai' dinormalisasi ke dalam rentang 0 hingga 1 agar lebih sesuai untuk pelatihan model. Setelah itu, data dibagi menjadi dua subset, yakni 80% untuk data pelatihan dan 20% untuk data validasi yang akan digunakan dalam proses training model.
"""

df = df.sample(frac=1, random_state=42)
df

x = df[['user', 'prodi']].values

y = df['rata_rata_nilai'].apply(lambda x: (x - min_nilai_mah) / (max_nilai_mah - min_nilai_mah)).values

# Membagi menjadi 80% data train dan 20% data validasi
train_indices = int(0.8 * df.shape[0])
x_train, x_val, y_train, y_val = (
    x[:train_indices],
    x[train_indices:],
    y[:train_indices],
    y[train_indices:]
)

print(x, y)

"""# Model Development | Content Based Filtering

# Cosine Similarity
Tujuan dari tahap Kesamaan cosinus adalah mengukur kesamaan antara dua vektor dan menentukan apakah mereka mengarah ke arah yang sama. Tahap kesamaan kosinus penting dalam model content-based filtering karena memberikan cara yang efektif untuk mengukur kesamaan antara dua vektor yang mewakili item-item dengan menghitung sudut kosinusnya.
"""

# Menghitung cosine similarity pada matrix tf-idf
cosine_sim = cosine_similarity(tfidf_matrix)
cosine_sim

# Membuat dataframe dari variabel cosine_sim dengan baris dan kolom berupa nama resto
cosine_sim_df = pd.DataFrame(cosine_sim, index=data['id_major'], columns=data['id_major'])
print('Shape:', cosine_sim_df.shape)

# Melihat similarity matrix pada setiap resto
cosine_sim_df.sample(5, axis=1).sample(10, axis=0)

"""**Insight**

Dari hasil perhitungan cosine similarity, diperoleh pemahaman mengenai tingkat kemiripan antar program studi berdasarkan representasi fitur mereka. Hal ini memungkinkan sistem untuk merekomendasikan jurusan lain yang memiliki karakteristik serupa, meskipun belum pernah dipilih oleh pengguna sebelumnya. Pendekatan ini efektif untuk mengatasi masalah cold start dan memperkaya sistem rekomendasi berbasis konten.

# Top-N Rekomendasi
Membuat sebuah fungsi yang akan menghasilkan dataframe berisi 5 jurusan teratas sebagai hasil rekomendasi.
"""

def major_recommendations(id_major, similarity_data=cosine_sim_df, items=data[['id_major', 'university_name', 'major_name']], k=5):

    index = similarity_data.loc[:,id_major].to_numpy().argpartition(
        range(-1, -k, -1))

    # Mengambil data dengan similarity terbesar dari index yang ada
    closest = similarity_data.columns[index[-1:-(k+2):-1]]
    closest = closest.drop(id_major, errors='ignore')


    recomen = pd.DataFrame(closest).merge(items).head(k)
    return recomen

data[data.id_major.eq(3612135)]

data[data.id_major.eq(7112114)]

"""Setelah itu, dilakukan pencarian untuk menampilkan semua data yang memiliki id_major 7112114"""

major_recommendations(3612135)

major_recommendations(7112114)

"""langkah selanjutnya adalah memberikan rekomendasi jurusan (major) yang mirip atau terkait dengan jurusan yang memiliki id_major 7112114 berdasarkan kemiripan isi jurusan (cosine similarity)."""

def precision_at_k(recommended_ids, relevant_ids, k):
    recommended_at_k = recommended_ids[:k]
    relevant_recommended = [id_ for id_ in recommended_at_k if id_ in relevant_ids]
    precision = len(relevant_recommended) / k
    return precision

relevant_ids = [3562281, 3232135, 3562242, 3622296]
recommended_ids = [3562281, 3232135, 3562242, 3622296, 3332144]

precision = precision_at_k(recommended_ids, relevant_ids, k=5)
print("Precision:", precision)

"""# Model Development | Collaborative Filtering

# 1. Generate Class RecommenderNet
Pada tahap ini, kelas RecommenderNetakan didefinisikan sebagai model neural network untuk sistem rekomendasi.
"""

class RecommenderNet(tf.keras.Model):

  # Insialisasi fungsi
  def __init__(self, num_user, num_prodi, embedding_size, **kwargs):
    super(RecommenderNet, self).__init__(**kwargs)
    self.num_user = num_user
    self.num_prodi = num_prodi
    self.embedding_size = embedding_size
    self.user_embeddings = layers.Embedding( # layer embedding user
        num_user,
        embedding_size,
        embeddings_initializer = 'he_normal',
        embeddings_regularizer = keras.regularizers.l2(1e-6)
    )
    self.user_bias = layers.Embedding(num_user, 1) # layer embedding user bias
    self.prodi_embedding = layers.Embedding( # layer embeddings books
        num_prodi,
        embedding_size,
        embeddings_initializer = 'he_normal',
        embeddings_regularizer = keras.regularizers.l2(1e-6)
    )
    self.prodi_bias = layers.Embedding(num_prodi, 1) # layer embedding books bias

  def call(self, inputs):
    user_vector = self.user_embeddings(inputs[:,0]) # memanggil layer embedding 1
    user_bias = self.user_bias(inputs[:, 0]) # memanggil layer embedding 2
    prodi_vector = self.prodi_embedding(inputs[:, 1]) # memanggil layer embedding 3
    prodi_bias = self.prodi_bias(inputs[:, 1]) # memanggil layer embedding 4

    dot_user_prodi = tf.tensordot(user_vector, prodi_vector, 2)

    x = dot_user_prodi+ user_bias + prodi_bias

    return tf.nn.sigmoid(x) # activation sigmoid

"""# 2. Compile Model
Model RecommenderNet yang telah dirancang sebelumnya diinisialisasi dengan parameter berupa jumlah pengguna (num_user), jumlah program studi (num_prodi), serta ukuran embedding sebesar 50. Setelah itu, model dikompilasi dengan menggunakan fungsi kerugian BinaryCrossentropy, optimizer Adam, dan metrik evaluasi Root Mean Squared Error (RMSE). Dengan konfigurasi tersebut, model siap untuk menjalani proses pelatihan menggunakan data yang tersedia.
"""

# Compile model
model = RecommenderNet(num_user, num_prodi, 50) # inisialisasi model

# model compile
model.compile(
    loss = tf.keras.losses.BinaryCrossentropy(),
    optimizer = keras.optimizers.Adam(),
    metrics=[tf.keras.metrics.RootMeanSquaredError()]
)

"""# 3. Callbacks
Melakukan penerapan fungsi callbacks berupa ReduceLROnPlateau() dan EarlyStopping() guna meningkatkan efektivitas serta efisiensi dalam proses pelatihan model.
"""

reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(
    monitor='val_loss',
    factor=0.2,
    patience=5,
    min_lr=1.5e-5
)

early_stop = tf.keras.callbacks.EarlyStopping(
    monitor="val_loss",
    min_delta=0,
    patience=12,
    verbose=0,
    mode="auto",
    baseline=None,
    restore_best_weights=True
)

"""# 4. Training Model
Pada tahap ini, dilakukan pelatihan model jaringan saraf (neural network) dengan menggunakan metode fit() pada objek model yang sudah melalui proses kompilasi sebelumnya.
"""

# Memulai training
history = model.fit(
    x = x_train,
    y = y_train,
    batch_size = 64,
    epochs = 100,
    validation_data = (x_val, y_val),
    callbacks = [reduce_lr, early_stop]
)

"""# 5. Top-N Recommendation
Proses ini dimulai dengan pengambilan sampel data secara acak untuk mengenali program studi (prodi) yang sudah dipilih oleh pengguna. Selanjutnya, prodi yang belum dipilih disaring agar hanya yang relevan dan terdaftar dalam kamus encoding yang dipertimbangkan. Kemudian, model melakukan prediksi skor untuk setiap kombinasi pengguna dan prodi yang belum dipilih, dan 10 prodi dengan skor tertinggi dipilih sebagai rekomendasi. Rekomendasi ini ditampilkan bersama dengan daftar prodi yang sudah dipilih oleh pengguna sebagai konteks tambahan.

Selanjutnya, model collaborative filtering yang sebelumnya telah dilatih digunakan untuk memprediksi skor pada prodi yang belum dipilih oleh pengguna. Dari hasil prediksi tersebut, 10 prodi dengan skor tertinggi diambil sebagai rekomendasi. Daftar rekomendasi ini disajikan bersama dengan prodi yang telah dipilih sebelumnya, lengkap dengan nama universitas masing-masing, untuk memberikan gambaran yang lebih jelas dan membantu pengguna dalam membuat keputusan yang tepat.
"""

# Mengambil sample user
user_id = df['id_user'].sample(1).iloc[0]
prodi_pick_by_user = df[df['id_user'] == user_id]
prodi_pick_by_user

prodi_not_pick = df[~df['id_major'].isin(prodi_pick_by_user['id_major'].values)]['id_major']
prodi_not_pick = list(
    set(prodi_not_pick)
    .intersection(set(prodi_to_prodi_encoded.keys()))
)
prodi_not_pick

prodi_not_pick = [[prodi_to_prodi_encoded.get(x)] for x in prodi_not_pick]
user_encoder = user_to_user_encoded.get(user_id)
user_prodi_array = np.hstack(
    ([[user_encoder]] * len(prodi_not_pick), prodi_not_pick)
)

user_prodi_array

ratings_model = model.predict(user_prodi_array).flatten()

top_ratings_indices = ratings_model.argsort()[-10:][::-1]

recommended_prodi_ids = [
    prodi_encoded_to_prodi.get(prodi_not_pick[x][0]) for x in top_ratings_indices
]
print('Memperlihatkan rekomendasi untuk users: {}'.format(user_id))
print('===' * 9)
print('Prodi berdasarkan input user')
print('----' * 8)

top_prodi_user = (
    prodi_pick_by_user.sort_values(
        by = 'rata_rata_nilai',
        ascending=False
    ).head().id_major.values
)

prodi_df_rows = df[df['id_major'].isin(top_prodi_user)]
for row in prodi_df_rows.itertuples():
    print(row.major_name, ':', row.university_name)

print('----' * 8)
print('Top 10 univ rekomendasi untuk user')
print('----' * 8)

recommended_univ = df[df['id_major'].isin(recommended_prodi_ids)]
for row in recommended_univ.itertuples():
    print(row.major_name, ':', row.university_name)
recommended_univ

"""# 6. Visualisasi Metrik"""

plt.plot(history.history['root_mean_squared_error'])
plt.plot(history.history['val_root_mean_squared_error'])
plt.title('model_metrics')
plt.ylabel('root_mean_squared_error')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()